{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_dataset(dataset, val_split=0.1):\n",
    "    train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=val_split)\n",
    "    return dict({\n",
    "        \"train\" : Subset(dataset, train_idx),\n",
    "        \"val\" : Subset(dataset, val_idx)\n",
    "    })\n",
    "\n",
    "def createDataLoader():\n",
    "    mean_values = (0.5, 0.5, 0.5)\n",
    "    std_values = (0.5, 0.5, 0.5)\n",
    "    op_size = 32\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(op_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean_values,std_values ),\n",
    "        ])\n",
    "\n",
    "    transform_horizontal = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(op_size),                                        \n",
    "        transforms.RandomHorizontalFlip(),                                       \n",
    "        transforms.ToTensor(),\n",
    "        ])\n",
    "    transform_vertical = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(op_size),                                     \n",
    "        transforms.RandomVerticalFlip(),                                       \n",
    "        transforms.ToTensor(),\n",
    "        ])\n",
    "    transform_Invert= transforms.Compose([\n",
    "        transforms.RandomResizedCrop(op_size),                                  \n",
    "        transforms.RandomInvert(),                                       \n",
    "        transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    dataset_url = \"/content/train_local/inaturalist_12K/train\"\n",
    "\n",
    "    img_data = torchvision.datasets.ImageFolder(root= dataset_url,  transform=transform_train)\n",
    "    img_data_hori= torchvision.datasets.ImageFolder(root= dataset_url,  transform=transform_horizontal)\n",
    "    img_data_vert= torchvision.datasets.ImageFolder(root= dataset_url,  transform=transform_vertical)\n",
    "    img_data_inve= torchvision.datasets.ImageFolder(root= dataset_url,  transform=transform_Invert)\n",
    "    img_data = img_data + img_data_inve + img_data_vert + img_data_hori\n",
    "\n",
    "    img_data = train_val_dataset(img_data)\n",
    "\n",
    "\n",
    "    X_train=img_data['train']\n",
    "    X_Valid=img_data['val']\n",
    "\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(X_train, batch_size=128, shuffle=True)\n",
    "    validationloader = torch.utils.data.DataLoader(X_Valid, batch_size=128, shuffle=False)\n",
    "\n",
    "\n",
    "    dataiter = iter(trainloader)\n",
    "    images, labels = dataiter.next()\n",
    "    return trainloader,validationloader\n",
    "\n",
    "\n",
    "def accuracy(dataset_itr,model,norm_fact):\n",
    "    total = 0\n",
    "    pred_count = 0\n",
    "    for dataset in dataset_itr:\n",
    "        X,y=dataset\n",
    "\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        pred = torch.max(model(X,norm_fact).data,1)[1]\n",
    "\n",
    "        total+=y.size(0)\n",
    "        pred_count+=(pred==y).sum().item()\n",
    "        acc = (100*pred_count)/total\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnModel(nn.Module):\n",
    "    def __init__(self,is_batch_norm,filter_size,file_org,F,d,actication_fun):\n",
    "        super(CnnModel, self).__init__()\n",
    "        if is_batch_norm:\n",
    "            self.cnn_model = nn.Sequential(\n",
    "                nn.Conv2d(3,filter_size,F),\n",
    "                nn.BatchNorm2d(int(filter_size)),\n",
    "                actication_fun,\n",
    "                nn.MaxPool2d(2,stride=1),  \n",
    "                \n",
    "                nn.Conv2d(filter_size,int(filter_size*file_org),F), \n",
    "                nn.BatchNorm2d(int(filter_size*(file_org**1))),\n",
    "                actication_fun,\n",
    "                nn.MaxPool2d(2,stride=1), \n",
    "                \n",
    "                nn.Conv2d(int(filter_size*file_org),int(filter_size*(file_org**2)),F), \n",
    "                nn.BatchNorm2d(int(filter_size*(file_org**2))),\n",
    "                actication_fun,\n",
    "                nn.MaxPool2d(2,stride=1), \n",
    "                            \n",
    "                nn.Conv2d(int(filter_size*(file_org**2)),int(filter_size*(file_org**3)),F),    \n",
    "                nn.BatchNorm2d(int(filter_size*(file_org**3))),\n",
    "                actication_fun,\n",
    "                nn.MaxPool2d(2,stride=2), \n",
    "                \n",
    "                nn.Conv2d(int(filter_size*(file_org**3)),int(filter_size*(file_org**4)),int(F)), \n",
    "                nn.BatchNorm2d(int(filter_size*(file_org**4))),\n",
    "                actication_fun,\n",
    "                nn.MaxPool2d(2,stride=2), \n",
    "            \n",
    "            )\n",
    "        else:\n",
    "            self.cnn_model = nn.Sequential(\n",
    "            nn.Conv2d(3,filter_size,F),\n",
    "            actication_fun,\n",
    "            nn.MaxPool2d(2,stride=1),\n",
    "\n",
    "            nn.Conv2d(filter_size,int(filter_size*file_org),F),           \n",
    "            actication_fun,\n",
    "            nn.MaxPool2d(2,stride=1), \n",
    "            \n",
    "            nn.Conv2d(int(filter_size*file_org),int(filter_size*(file_org**2)),F),\n",
    "            actication_fun,\n",
    "            nn.MaxPool2d(2,stride=1), \n",
    "\n",
    "            nn.Conv2d(int(filter_size*(file_org**2)),int(filter_size*(file_org**3)),F),\n",
    "            actication_fun,\n",
    "            nn.MaxPool2d(2,stride=2),\n",
    "\n",
    "            nn.Conv2d(int(filter_size*(file_org**3)),int(filter_size*(file_org**4)),int(F)),\n",
    "            actication_fun,\n",
    "            nn.MaxPool2d(2,stride=2),\n",
    "\n",
    "            output_dim=x_dim-self.F+1,\n",
    "\n",
    "            )\n",
    "\n",
    "\n",
    "        x_dim=32\n",
    "        y_dim=x_dim-F+1  \n",
    "\n",
    "        x_dim=y_dim-2   \n",
    "        y_dim=x_dim-F+1  \n",
    "        y_dim=y_dim-1 \n",
    "\n",
    "        x_dim=y_dim\n",
    "        y_dim=x_dim-F+1 \n",
    "        y_dim=y_dim-1  \n",
    "\n",
    "        x_dim=y_dim\n",
    "        y_dim=x_dim-F+1 \n",
    "        y_dim=int(y_dim/2) \n",
    "        \n",
    "        x_dim=y_dim\n",
    "        y_dim=x_dim-F+1  \n",
    "\n",
    "        y_dim=y_dim/2\n",
    "        y_dim=int(y_dim)\n",
    " \n",
    "        self.fc_model = nn.Sequential(\n",
    "           nn.Linear(int(y_dim*y_dim*filter_size*(file_org**4)),120),\n",
    "           nn.ReLU(),\n",
    "           nn.Dropout(d),\n",
    "           nn.Linear(120,10)\n",
    "        )\n",
    "    def forward(self, x,batch_norm):\n",
    "        x = self.cnn_model(x)\n",
    "        if batch_norm:\n",
    "            x = self.cnn_model(x)\n",
    "        return self.fc_model(x.view(x.size(0), -1))\n",
    "\n",
    "      \n",
    "def fit(batch_norm,k,file_org,F,max_epochs,batch_size,d,act_fun=nn.ReLU()):\n",
    "    model = CnnModel(is_batch_norm = batch_norm,\n",
    "                        filter_size = k,\n",
    "                        file_org = file_org,\n",
    "                        F = F,\n",
    "                        d = d,\n",
    "                        actication_fun = act_fun).to(device)\n",
    "    opt = optim.Adam(model.parameters())\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    train_loss,val_loss=[],[]\n",
    "    \n",
    "    n_iter=np.ceil(8999/batch_size)\n",
    "    trainloader,validationloader = createDataLoader()\n",
    "    epoch = 0\n",
    "    while epoch<max_epochs:\n",
    "        for key,data in enumerate(trainloader,0):\n",
    "            X,y=data\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            opt.zero_grad()\n",
    "\n",
    "            outputs=model(X,batch_norm) \n",
    "            loss=loss_fn(outputs,y)\n",
    "            loss.backward()\n",
    "\n",
    "            opt.step()\n",
    "        \n",
    "            del X,y,outputs\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "            if(key%100==0):\n",
    "                print(f\"Iter No. : {key}/{n_iter} , loss: {round(loss.item(),2)} \")\n",
    "        \n",
    "        \n",
    "        \n",
    "        for key,data in enumerate(validationloader,0):\n",
    "            X,y=data\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            outputs = model(X,batch_norm)\n",
    "            loss = loss_fn(outputs,y)\n",
    "\n",
    "            del X\n",
    "            del y\n",
    "            del outputs\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        epoch+=1\n",
    "    val_loss.append(loss.item()) \n",
    "    train_loss.append(loss.item()) \n",
    "  \n",
    "    print(\"Training_accuracy:%.2f\" % (accuracy(trainloader,model,batch_norm)))\n",
    "    print(\"Validation_accuracy:%.2f\" % (accuracy(validationloader,model,batch_norm)))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
